{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da39dc88-f3da-42de-a6cf-6d291602d011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.aa.com.tr/tr/ekonomi/lng-talebindeki-dusus-yunanistanin-terminal-projelerini-olumsuz-etkileyebilir/3297386\n",
      "2024-07-08-00-00-00\n",
      "https://www.aa.com.tr/tr/ekonomi/ekonomide-gozler-enflasyonla-mucadele-adimlari-ve-buyume-verilerinde/3297379\n",
      "2024-07-08-00-00-00\n",
      "https://www.aa.com.tr/tr/ekonomi/kuresel-piyasalardaki-toparlanma-egilimi-surerken-vix-endeksi-27-71-ye-geriledi/3297343\n",
      "2024-07-08-00-00-00\n",
      "https://www.aa.com.tr/tr/ekonomi/turkiyeye-gelecek-2-5-milyar-dolarlik-dis-finansman-son-asamada/3297317\n",
      "2024-07-08-00-00-00\n",
      "https://www.aa.com.tr/tr/ekonomi/borsa-gune-yukselisle-basladi/3297313\n",
      "2024-07-08-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-01-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-02-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n",
      "2024-03-07-00-00-00\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:761\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# Invalid chunked protocol response, abort.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 16: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidChunkLength\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:444\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:765\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 765\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidChunkLength(\u001b[38;5;28mself\u001b[39m, line)\n",
      "\u001b[0;31mInvalidChunkLength\u001b[0m: InvalidChunkLength(got length b'', 0 bytes read)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:857\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_response:\n\u001b[0;32m--> 857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_response\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:131\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py:461\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e, e)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 247\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    246\u001b[0m     scraper \u001b[38;5;241m=\u001b[39m DataScraper()\n\u001b[0;32m--> 247\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_all_sites\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     old_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myeni_türkçe_haberler.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    250\u001b[0m     combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([old_data, data], ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 240\u001b[0m, in \u001b[0;36mDataScraper.scrape_all_sites\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m df_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malltext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    239\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maa_news_links(), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 240\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_bloomberght_news_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# 12345 azalt\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_data\n",
      "Cell \u001b[0;32mIn[12], line 157\u001b[0m, in \u001b[0;36mDataScraper.scrape_bloomberght_news_links\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m new_rows_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malltext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m--> 157\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup_maker\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     dom \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mHTML(\u001b[38;5;28mstr\u001b[39m(soup))\n\u001b[1;32m    161\u001b[0m     before_new_rows_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malltext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m, in \u001b[0;36mDataScraper.soup_maker\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoup_maker\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[0;32m---> 33\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m soup\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/sessions.py:745\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 745\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import warnings\n",
    "from dateutil.parser import parse\n",
    "import datetime as dt\n",
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "\n",
    "class DataScraper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        client = pymongo.MongoClient('127.0.0.1', 27017)\n",
    "        db = client[\"news\"]\n",
    "        newsCollection = db[\"news_collection\"]\n",
    "\n",
    "        self.proxies = {\n",
    "            \"https\" : \"fnyproxy.fnylocal.com:8080\",\n",
    "            \"http\" : \"fnyproxy.fnylocal.com:8080\"\n",
    "        }\n",
    "\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0'}\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def soup_maker(self, url):\n",
    "\n",
    "        res = requests.get(url, proxies = self.proxies,headers=self.headers, verify = False)\n",
    "\n",
    "        soup = BeautifulSoup(res.content.decode('utf-8'), \"lxml\")\n",
    "        return soup\n",
    "    \n",
    "    def turkce_aylar(self, ay):\n",
    "        ham_ay = ay[2:len(ay)-5]\n",
    "        turkce = re.sub('[0-9 ]', '', ay)\n",
    "        ing_dict = {'Ocak': ' January', 'Şubat': ' February', 'Mart': ' March', 'Nisan': ' April', 'Mayıs': ' May', 'Haziran': ' June', 'Temmuz': ' July', 'Ağustos': ' August',\n",
    "            'Eylül': ' September', 'Ekim': ' October', 'Kasım': ' November', 'Aralık': ' December' }\n",
    "\n",
    "        return ay.replace(ham_ay,ing_dict[turkce])\n",
    "        \n",
    "    ####### AA ####### //div/div/header/h1[contains(@class, \"text-3xl font-medium tracking-tight text-gray-1100 leading-relative-2 md:text-5xl\")]\n",
    "    def aa_news_links(self):\n",
    "        \n",
    "        \n",
    "        url = \"https://www.aa.com.tr/tr/ekonomi\"\n",
    "\n",
    "\n",
    "\n",
    "        soup = self.soup_maker(url)\n",
    "\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        \n",
    "        links_element = dom.xpath('//div/div[contains(@class, \"row konu-alt-icerik\")]/a')[:-1]\n",
    "\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "        for link_ele in links_element:\n",
    "            combine_whole_link =  link_ele.get('href')\n",
    "            print(combine_whole_link)\n",
    "            \n",
    "            title, text, date = self.go_inside_aa_links(link = combine_whole_link)\n",
    "            \n",
    "            new_row = {\"headline\": title, \"alltext\": text, \"date\": date, \"source\": \"AA\", \"url\": combine_whole_link}\n",
    "            new_rows_df = new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "    \n",
    "    \n",
    "    def go_inside_aa_links(self, link):\n",
    "        # for link in links: \n",
    "        soup = self.soup_maker(link)\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        title_element = dom.xpath('//div/div[contains(@class, \"detay-spot-category\")]/h1/text()')\n",
    "        if title_element != []:\n",
    "            title = title_element[0]\n",
    "        else:\n",
    "            title = title_element\n",
    "        # title = 'titlegelecek'\n",
    "        \n",
    "        text_element =dom.xpath('//div/div[contains(@class, \"detay-icerik\")]/div/p/text()')\n",
    "        text = ''\n",
    "        self.text_element = text_element\n",
    "        for paragraph in text_element:\n",
    "            text = text + paragraph\n",
    "        # text= 'textgelecek'\n",
    "\n",
    "        date_element = dom.xpath('//div/div[contains(@class, \"detay-spot-category\")]/div/span[contains(@class, \"tarih\")]/text()')[0]\n",
    "        date = date_element\n",
    "        date = date[-10:]\n",
    "        \n",
    "        monthDict={\"01\":'Jan', \"02\":'Feb', \"03\":'Mar', \"04\":'Apr', \"05\":'May', \"06\":'Jun', \"07\":'Jul', \"08\":'Aug',\"09\":'Sep', \"10\":'Oct', \"11\":'Nov', \"12\":'Dec'}\n",
    "\n",
    "        ay_num = date.split('.')[1]\n",
    "\n",
    "        date = sda.replace(ay_num, monthDict[ay_num])\n",
    "        \n",
    "\n",
    "        \n",
    "        date = pd.to_datetime(date)\n",
    "        date = date.strftime('%Y-%d-%m-%H-%M-%S')\n",
    "\n",
    "        print(date)\n",
    "        # date = '13'\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "            \n",
    "        \n",
    "    ######################\n",
    "        \n",
    "    ####### bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght bloomberght #######\n",
    "    \n",
    "    \n",
    "    def urls_bloomberght(self):    ############################33 cok fazla link var bloomberght\n",
    "        main_url = \"https://www.bloomberght.com/sitemap.xml\" \n",
    "        soup = self.soup_maker(main_url)\n",
    "        big_urls = []\n",
    "    #     dom = etree.HTML(str(soup))\n",
    "\n",
    "    #     links_element = dom.xpath('//loc/text()')\n",
    "    #     print(links_element)\n",
    "        urls = soup.find_all('loc')\n",
    "        for url in urls:\n",
    "            big_urls.append(url.text)\n",
    "\n",
    "        # print(big_urls)\n",
    "        all_urls = []\n",
    "        #\n",
    "        big_urls = big_urls[6:7] ##############################  azalmak için yoksa time hata verebilir bloomberght\n",
    "        #\n",
    "        for big in big_urls:\n",
    "            soup = self.soup_maker(big)\n",
    "            a_urls = soup.find_all('loc')\n",
    "\n",
    "            for a_url in a_urls:\n",
    "                all_urls.append(a_url.text)\n",
    "\n",
    "        return all_urls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_bloomberght_news_links(self):\n",
    "\n",
    "        urls = self.urls_bloomberght()\n",
    "        new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "        for url in urls:\n",
    "            soup = self.soup_maker(url)\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            before_new_rows_df = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "\n",
    "\n",
    "\n",
    "            title, text, date = self.go_inside_bloomberght_links(link = url)\n",
    "\n",
    "\n",
    "\n",
    "            new_row = {\"headline\": title, \"alltext\": text, \"date\": date, \"source\": \"bloomberght\", \"url\": url}\n",
    "            before_new_rows_df = before_new_rows_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            new_rows_df = new_rows_df.append(before_new_rows_df, ignore_index=True)\n",
    "\n",
    "        return new_rows_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def go_inside_bloomberght_links(self, link):\n",
    "        # for link in links:\n",
    "        soup = self.soup_maker(url = link)\n",
    "\n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        title_element = dom.xpath('//div/div/section/figure/figcaption/h1/span/text()')\n",
    "        if title_element != []:\n",
    "            title = title_element[0]\n",
    "        else:\n",
    "            title = title_element\n",
    "        # title = 'titlegelecekqqq'\n",
    "\n",
    "        text_element =dom.xpath('//div/div/article/div[contains(@class, \"pagebreak\")]/p/text()')\n",
    "        text = ''\n",
    "        text_element = text_element\n",
    "        for paragraph in text_element:\n",
    "            text = text + paragraph\n",
    "\n",
    "        # paragraphs = text_element.find_all('p') if text_element else []\n",
    "        # text = '\\n'.join(p.get_text() for p in paragraphs)\n",
    "        # text= 'textgelecekq1234'\n",
    "\n",
    "        date_element = dom.xpath('//div/div/section/figure/figcaption/div/span/time/text()')\n",
    "        # print(date_element)\n",
    "        date_element = list(date_element)\n",
    "        try:\n",
    "\n",
    "            date = date_element[0]\n",
    "            date = date[:-7]\n",
    "            date = date.rsplit(' ', 1)[0]\n",
    "            # date = date.encode('utf-8')\n",
    "            # date = str(date)[2:len(str(date))-1]\n",
    "            date = self.turkce_aylar(date)\n",
    "\n",
    "            date = pd.to_datetime(date)\n",
    "            date = date.strftime('%Y-%d-%m-%H-%M-%S')\n",
    "\n",
    "        except:\n",
    "            date = date_element\n",
    "\n",
    "        print(date)\n",
    "        # date = '13'\n",
    "\n",
    "\n",
    "        return title, text, date\n",
    "\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def scrape_all_sites(self):\n",
    "        # self.scrape_bloomberght_news_links()\n",
    "        df_data = pd.DataFrame(columns = [\"headline\", \"alltext\", \"date\", \"source\", \"url\"])\n",
    "        df_data = df_data.append(self.aa_news_links(), ignore_index=True)\n",
    "        df_data = df_data.append(self.scrape_bloomberght_news_links(), ignore_index=True)  # 12345 azalt\n",
    "        return df_data\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    scraper = DataScraper()\n",
    "    data = scraper.scrape_all_sites()\n",
    "    old_data = pd.read_excel(\"yeni_türkçe_haberler.xlsx\", engine = 'openpyxl')\n",
    "    \n",
    "    combined_data = pd.concat([old_data, data], ignore_index = True)\n",
    "    \n",
    "    for col in combined_data.columns:\n",
    "        if any(isinstance(x, list) for x in combined_data[col]):\n",
    "            combined_data[col] = combined_data[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "    final_data = combined_data.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    final_data.to_excel(\"yeni_türkçe_haberler.xlsx\", index = False, engine = 'openpyxl')\n",
    "\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05557076-a8fe-4c70-a6f6-72dcf8a470d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "4+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1eb8f983-a3cd-4f19-ad56-76cb56fde368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss = '06 Ağustos 2024 07:23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1131ed0a-bdbc-48eb-8708-a5f48885a247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = ss[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68ccb7d4-6423-45e4-9ab9-1ebb82c17b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'August'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s1 = re.sub('[0-9 ]', '', s)\n",
    "\n",
    "\n",
    "ing_dict = {'Ocak': 'January', 'Şubat': 'February', 'Mart': 'March', 'Nisan': 'April', 'Mayıs': 'May', 'Haziran': 'June', 'Temmuz': 'July', 'Ağustos': 'August',\n",
    "    'Eylül': 'September', 'Ekim': 'October', 'Kasım': 'November', 'Aralık': 'December'}\n",
    "\n",
    "ing_dict[s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e342b9b8-6aa1-457a-a457-dd206943eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthDict={\"01\":'Jan', \"02\":'Feb', \"03\":'Mar', \"04\":'Apr', \"05\":'May', \"06\":'Jun', \"07\":'Jul', \"08\":'Aug',\"09\":'Sep', \"10\":'Oct', \"11\":'Nov', \"12\":'Dec'}\n",
    "\n",
    "date = '7.08.2024'\n",
    "ay_num = date.split('.')[1]\n",
    "\n",
    "date = sda.replace(ay_num, monthDict[ay_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19093e6c-79cb-471e-817b-e516fdac99a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.Aug.2024'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f739b-01b1-4969-8cdb-7662b2fc5589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
